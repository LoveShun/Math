\BOOKMARK [1][-]{section.1}{前言}{}% 1
\BOOKMARK [1][-]{section.2}{线性回归}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{假设函数\(Hypothesis Function\)}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{最小二乘法}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{梯度下降}{section.2}% 5
\BOOKMARK [3][-]{subsubsection.2.3.1}{批梯度下降\(BGD\)}{subsection.2.3}% 6
\BOOKMARK [3][-]{subsubsection.2.3.2}{随机梯度下降\(SGD\)}{subsection.2.3}% 7
\BOOKMARK [3][-]{subsubsection.2.3.3}{迷你批梯度下降\(mini-batch GD\)}{subsection.2.3}% 8
\BOOKMARK [2][-]{subsection.2.4}{线性代数知识}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.5}{梯度下降过程的矩阵表达}{section.2}% 10
\BOOKMARK [2][-]{subsection.2.6}{线性回归中使用最小二乘法的合理性解释}{section.2}% 11
\BOOKMARK [3][-]{subsubsection.2.6.1}{证明过程}{subsection.2.6}% 12
\BOOKMARK [3][-]{subsubsection.2.6.2}{说明}{subsection.2.6}% 13
\BOOKMARK [1][-]{section.3}{局部加权回归\(LWR：Local Weight Regression\)}{}% 14
\BOOKMARK [2][-]{subsection.3.1}{概念}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.2}{计算方法}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.3}{其他注意事项}{section.3}% 17
\BOOKMARK [1][-]{section.4}{逻辑回归}{}% 18
\BOOKMARK [2][-]{subsection.4.1}{sigmoid\040Function}{section.4}% 19
\BOOKMARK [2][-]{subsection.4.2}{假设函数h\(x\)\(Hypothesis Function\)}{section.4}% 20
\BOOKMARK [2][-]{subsection.4.3}{推导过程}{section.4}% 21
\BOOKMARK [2][-]{subsection.4.4}{逻辑回归&线性回归}{section.4}% 22
\BOOKMARK [2][-]{subsection.4.5}{0-1分布&逻辑回归}{section.4}% 23
\BOOKMARK [2][-]{subsection.4.6}{感知器算法}{section.4}% 24
\BOOKMARK [2][-]{subsection.4.7}{牛顿方法}{section.4}% 25
\BOOKMARK [3][-]{subsubsection.4.7.1}{牛顿方法的思路}{subsection.4.7}% 26
\BOOKMARK [3][-]{subsubsection.4.7.2}{牛顿方法讲解}{subsection.4.7}% 27
\BOOKMARK [1][-]{section.5}{广义线性模型}{}% 28
\BOOKMARK [2][-]{subsection.5.1}{指数分布族}{section.5}% 29
\BOOKMARK [2][-]{subsection.5.2}{伯努利分布与高斯分布中，GLM各部分的值}{section.5}% 30
\BOOKMARK [2][-]{subsection.5.3}{如何构造广义线性模型}{section.5}% 31
\BOOKMARK [3][-]{subsubsection.5.3.1}{三个假设}{subsection.5.3}% 32
\BOOKMARK [3][-]{subsubsection.5.3.2}{普通的最小二乘法}{subsection.5.3}% 33
\BOOKMARK [3][-]{subsubsection.5.3.3}{逻辑回归}{subsection.5.3}% 34
\BOOKMARK [3][-]{subsubsection.5.3.4}{说明}{subsection.5.3}% 35
\BOOKMARK [2][-]{subsection.5.4}{Softmax\040Regression}{section.5}% 36
\BOOKMARK [3][-]{subsubsection.5.4.1}{关于的说明}{subsection.5.4}% 37
\BOOKMARK [3][-]{subsubsection.5.4.2}{关于T\(y\)的表示法说明}{subsection.5.4}% 38
\BOOKMARK [3][-]{subsubsection.5.4.3}{证明多项式分布属于指数族分布}{subsection.5.4}% 39
\BOOKMARK [3][-]{subsubsection.5.4.4}{使用Softmax进行分类}{subsection.5.4}% 40
\BOOKMARK [3][-]{subsubsection.5.4.5}{参数的拟合方式}{subsection.5.4}% 41
\BOOKMARK [1][-]{section.6}{生成学习算法}{}% 42
\BOOKMARK [2][-]{subsection.6.1}{生成学习算法简介}{section.6}% 43
\BOOKMARK [3][-]{subsubsection.6.1.1}{判别学习算法&生成学习算法}{subsection.6.1}% 44
\BOOKMARK [2][-]{subsection.6.2}{高斯判别分析}{section.6}% 45
\BOOKMARK [3][-]{subsubsection.6.2.1}{多重正态分布简介}{subsection.6.2}% 46
\BOOKMARK [3][-]{subsubsection.6.2.2}{高斯判别分析模型}{subsection.6.2}% 47
\BOOKMARK [3][-]{subsubsection.6.2.3}{高斯判别分析&逻辑回归}{subsection.6.2}% 48
\BOOKMARK [2][-]{subsection.6.3}{朴素贝叶斯}{section.6}% 49
\BOOKMARK [2][-]{subsection.6.4}{拉普拉斯平滑}{section.6}% 50
\BOOKMARK [3][-]{subsubsection.6.4.1}{背景介绍}{subsection.6.4}% 51
\BOOKMARK [3][-]{subsubsection.6.4.2}{拉普拉斯平滑介绍}{subsection.6.4}% 52
\BOOKMARK [2][-]{subsection.6.5}{文本分类}{section.6}% 53
\BOOKMARK [1][-]{section.7}{附录}{}% 54
\BOOKMARK [2][-]{subsection.7.1}{概念与定义}{section.7}% 55
\BOOKMARK [3][-]{subsubsection.7.1.1}{各类变量}{subsection.7.1}% 56
\BOOKMARK [3][-]{subsubsection.7.1.2}{充分统计量}{subsection.7.1}% 57
\BOOKMARK [3][-]{subsubsection.7.1.3}{自然参数}{subsection.7.1}% 58
\BOOKMARK [3][-]{subsubsection.7.1.4}{先验概率 & 后验概率}{subsection.7.1}% 59
\BOOKMARK [3][-]{subsubsection.7.1.5}{似然函数}{subsection.7.1}% 60
\BOOKMARK [3][-]{subsubsection.7.1.6}{参数学习算法 & 非参数学习算法}{subsection.7.1}% 61
\BOOKMARK [2][-]{subsection.7.2}{中英对照表}{section.7}% 62
\BOOKMARK [2][-]{subsection.7.3}{思考}{section.7}% 63
