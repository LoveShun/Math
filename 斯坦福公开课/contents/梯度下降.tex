\section{假设函数(Hypothesis Function)}
\subsection{假设函数}
\begin{equation}\begin{aligned}
	h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ... + \theta_nx_n
\end{aligned}\end{equation}
另$x_0=1$，则
\begin{equation}\begin{aligned}
	h_\theta(x) &= \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ... + \theta_nx_n \\
	&= \sum_{j=0}^n{\theta_jx_j}
\end{aligned}\end{equation}

\section{梯度下降(GD)}
\subsection{Cost Function}
\begin{equation}\begin{aligned}
	J(\theta) = \frac{1}{2} \sum_{i=1}^m \left[h_{\theta} {(x^{(i)})} - y^{(i)}\right]^2
\end{aligned}\end{equation}
其中，$J(\theta)$就是在训练过程中的Cost Function。我们的目的就是将该Cost Function最小化。
在以上式子中：\\
	$x^{(i)}$为训练集的输入；\\
	$y^{(i)}$为训练集的输出；\\
	$\theta$为所要训练的参数；\\
	$h_{\theta}$为假设函数；\\
	$h_{\theta} {(x^{(i)})}$为训练集输入经过假设函数后得到的结果\\

\subsection{梯度下降(GD)}
\begin{equation}\begin{aligned}
      \theta_j &:= \theta_j - \alpha \frac{\partial} {\partial \theta_j} J(\theta)
\end{aligned}\end{equation}
其中：
\begin{equation}\begin{aligned}
      \frac{\partial} {\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{1}{2} \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right]^2 \\
      &= 2 * \frac{1}{2} \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right] \frac{\partial}{\partial\theta_j}\left[ h_\theta(x^{(i)}) - y^{(i)} \right] \\
      &= \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right]\frac{\partial}{\partial\theta_j}\left[ \theta_0x_0^{(i)} +  \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + ... + \theta_nx_n^{(i)} - y^{(i)} \right] \\
      &= \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right]x_j^{(i)}
\end{aligned}\end{equation}
所以，更新后的梯度下降公式为
\begin{equation}\begin{aligned}
	\theta_j &:= \theta_j - \alpha \sum_{i=1}^m \left[ h_\theta(x^{(i)}) - y^{(i)} \right]x_j^{(i)}
\end{aligned}\end{equation}
其中，$\alpha$称为学习速率(learning rate)，用于控制$\theta$前进的步伐，避免$\theta$走得太快（或太慢）。\\
由上式可以发现，在每一次梯度下降的迭代过程中，我们遍历了所有训练集。这将会耗费大量的性能，于是产生了随机梯度下降(SGD)方法，每次迭代只使用一个数据。
\\ Todo PS：批梯度下降指的是使用所有数据还是使用一部分数据待确认。


\section{随机梯度下降(SGD)}
\subsection{Cost Function}
\begin{equation}\begin{aligned}
	J(\theta) = \frac{1}{2} \left[h_{\theta} {(x^{(i)})} - y^{(i)}\right]^2
\end{aligned}\end{equation}

\subsection{随机梯度下降(SGD)}
\begin{equation}\begin{aligned}
      \frac{\partial} {\partial \theta_j} J(\theta) &= \left[ h_\theta(x^{(i)}) - y^{(i)} \right]x_j^{(i)}
\end{aligned}\end{equation}
更新后的梯度下降公式为
\begin{equation}\begin{aligned}
	\theta_j &:= \theta_j - \alpha\left[ h_\theta(x^{(i)}) - y^{(i)} \right]x_j^{(i)}
\end{aligned}\end{equation}
如上所示，与梯度下降不同，随机梯度下降每次迭代只用了一个数据（第$i$个数据），若$i$从1取到m，则完成了一次训练集的遍历。\\
使用随机梯度下降虽然解决了梯度下降耗费过多性能的问题，但是却带来了另一个问题：收敛太慢！由此，我们折中使用迷你批梯度下降(mini-batch GD)。\\
随机梯度下降（包括其他梯度下降方法）均允许多次遍历所有数据集。

\subsection{迷你批梯度下降(mini-batch GD)}
为了解决梯度下降每次都使用所有训练集导致的性能问题，以及随机梯度下降每次只使用一个数据导致的收敛太慢，我们可以只用mini-batch GD。每次只用一部分训练集进行训练。











