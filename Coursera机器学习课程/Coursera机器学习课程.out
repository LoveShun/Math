\BOOKMARK [1][-]{section.1}{线性回归\(Linear Regression\)}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{各向量形式}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{批梯度下降}{section.1}% 3
\BOOKMARK [3][-]{subsubsection.1.2.1}{各矩阵形式}{subsection.1.2}% 4
\BOOKMARK [3][-]{subsubsection.1.2.2}{Cost\040Function}{subsection.1.2}% 5
\BOOKMARK [3][-]{subsubsection.1.2.3}{偏导数J\(\)j计算}{subsection.1.2}% 6
\BOOKMARK [3][-]{subsubsection.1.2.4}{梯度下降迭代方式}{subsection.1.2}% 7
\BOOKMARK [2][-]{subsection.1.3}{Feature\040Normalization}{section.1}% 8
\BOOKMARK [2][-]{subsection.1.4}{公式法求解（Normal Equation）}{section.1}% 9
\BOOKMARK [1][-]{section.2}{逻辑回归\(Logistic Regression\)}{}% 10
\BOOKMARK [2][-]{subsection.2.1}{当只有2个类别时，使用1个分类器}{section.2}% 11
\BOOKMARK [3][-]{subsubsection.2.1.1}{sigmoid函数}{subsection.2.1}% 12
\BOOKMARK [3][-]{subsubsection.2.1.2}{预测函数}{subsection.2.1}% 13
\BOOKMARK [3][-]{subsubsection.2.1.3}{Cost\040Function}{subsection.2.1}% 14
\BOOKMARK [3][-]{subsubsection.2.1.4}{偏导数J\(\)j}{subsection.2.1}% 15
\BOOKMARK [3][-]{subsubsection.2.1.5}{梯度下降迭代算法}{subsection.2.1}% 16
\BOOKMARK [2][-]{subsection.2.2}{当只有k个类别时，使用k个分类器}{section.2}% 17
\BOOKMARK [1][-]{section.3}{Regularization}{}% 18
\BOOKMARK [2][-]{subsection.3.1}{线性回归}{section.3}% 19
\BOOKMARK [3][-]{subsubsection.3.1.1}{数值计算方式}{subsection.3.1}% 20
\BOOKMARK [3][-]{subsubsection.3.1.2}{矩阵计算方式}{subsection.3.1}% 21
\BOOKMARK [2][-]{subsection.3.2}{逻辑回归}{section.3}% 22
\BOOKMARK [3][-]{subsubsection.3.2.1}{数值计算方式}{subsection.3.2}% 23
\BOOKMARK [3][-]{subsubsection.3.2.2}{矩阵计算方式}{subsection.3.2}% 24
\BOOKMARK [2][-]{subsection.3.3}{注意}{section.3}% 25
\BOOKMARK [1][-]{section.4}{神经网络–前向算法}{}% 26
\BOOKMARK [2][-]{subsection.4.1}{神经网络示意图–前向算法}{section.4}% 27
\BOOKMARK [2][-]{subsection.4.2}{神经网络–前向算法}{section.4}% 28
\BOOKMARK [3][-]{subsubsection.4.2.1}{X、、、z、a}{subsection.4.2}% 29
\BOOKMARK [3][-]{subsubsection.4.2.2}{y}{subsection.4.2}% 30
\BOOKMARK [1][-]{section.5}{神经网络–后向算法}{}% 31
\BOOKMARK [2][-]{subsection.5.1}{神经网络示意图–后向算法}{section.5}% 32
\BOOKMARK [2][-]{subsection.5.2}{神经网络–后向算法}{section.5}% 33
\BOOKMARK [3][-]{subsubsection.5.2.1}{输出层结果：aL}{subsection.5.2}% 34
\BOOKMARK [3][-]{subsubsection.5.2.2}{格式化后的Y}{subsection.5.2}% 35
\BOOKMARK [3][-]{subsubsection.5.2.3}{L}{subsection.5.2}% 36
\BOOKMARK [3][-]{subsubsection.5.2.4}{L-1}{subsection.5.2}% 37
\BOOKMARK [3][-]{subsubsection.5.2.5}{l\(2<=l<=L-2\)}{subsection.5.2}% 38
\BOOKMARK [3][-]{subsubsection.5.2.6}{l（用迭代的方式计算）}{subsection.5.2}% 39
\BOOKMARK [3][-]{subsubsection.5.2.7}{Dij\(l\)}{subsection.5.2}% 40
\BOOKMARK [3][-]{subsubsection.5.2.8}{J\(\)ij\(l\)}{subsection.5.2}% 41
\BOOKMARK [3][-]{subsubsection.5.2.9}{l 与 l的区别与联系}{subsection.5.2}% 42
\BOOKMARK [1][-]{section.6}{调试技巧}{}% 43
\BOOKMARK [2][-]{subsection.6.1}{Error\040Analysis}{section.6}% 44
\BOOKMARK [2][-]{subsection.6.2}{Error\040Metrics\040for\040Skewed\040Classes}{section.6}% 45
\BOOKMARK [2][-]{subsection.6.3}{如何评价Precision与Recall}{section.6}% 46
\BOOKMARK [2][-]{subsection.6.4}{拟合效果不好时的解决方法指导}{section.6}% 47
\BOOKMARK [2][-]{subsection.6.5}{不同神经网络的优缺点}{section.6}% 48
\BOOKMARK [2][-]{subsection.6.6}{绘制Learning Curve}{section.6}% 49
\BOOKMARK [1][-]{section.7}{SVM}{}% 50
\BOOKMARK [2][-]{subsection.7.1}{Cost\040Function}{section.7}% 51
\BOOKMARK [2][-]{subsection.7.2}{Gaussian\040Kernel}{section.7}% 52
\BOOKMARK [2][-]{subsection.7.3}{SVM中，C与2对欠拟合或过拟合的影响}{section.7}% 53
\BOOKMARK [2][-]{subsection.7.4}{如何选项使用Logistic Regression还是 SVM}{section.7}% 54
