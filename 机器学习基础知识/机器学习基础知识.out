\BOOKMARK [1][-]{section.1}{线性回归}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{假设函数\(Hypothesis Function\)}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{最小二乘法}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{梯度下降}{section.1}% 4
\BOOKMARK [3][-]{subsubsection.1.3.1}{批梯度下降\(BGD\)}{subsection.1.3}% 5
\BOOKMARK [3][-]{subsubsection.1.3.2}{随机梯度下降\(SGD\)}{subsection.1.3}% 6
\BOOKMARK [3][-]{subsubsection.1.3.3}{迷你批梯度下降\(mini-batch GD\)}{subsection.1.3}% 7
\BOOKMARK [2][-]{subsection.1.4}{线性代数知识}{section.1}% 8
\BOOKMARK [2][-]{subsection.1.5}{梯度下降过程的矩阵表达}{section.1}% 9
\BOOKMARK [2][-]{subsection.1.6}{线性回归中使用最小二乘法的合理性解释}{section.1}% 10
\BOOKMARK [3][-]{subsubsection.1.6.1}{证明过程}{subsection.1.6}% 11
\BOOKMARK [3][-]{subsubsection.1.6.2}{说明}{subsection.1.6}% 12
\BOOKMARK [1][-]{section.2}{局部加权回归\(LWR：Local Weight Regression\)}{}% 13
\BOOKMARK [2][-]{subsection.2.1}{概念}{section.2}% 14
\BOOKMARK [2][-]{subsection.2.2}{计算方法}{section.2}% 15
\BOOKMARK [2][-]{subsection.2.3}{其他注意事项}{section.2}% 16
\BOOKMARK [1][-]{section.3}{逻辑回归}{}% 17
\BOOKMARK [2][-]{subsection.3.1}{sigmoid\040Function}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.2}{假设函数h\(x\)\(Hypothesis Function\)}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.3}{推导过程}{section.3}% 20
\BOOKMARK [2][-]{subsection.3.4}{逻辑回归&线性回归}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.5}{0-1分布&逻辑回归}{section.3}% 22
\BOOKMARK [2][-]{subsection.3.6}{感知器算法}{section.3}% 23
\BOOKMARK [2][-]{subsection.3.7}{牛顿方法}{section.3}% 24
\BOOKMARK [3][-]{subsubsection.3.7.1}{牛顿方法的思路}{subsection.3.7}% 25
\BOOKMARK [3][-]{subsubsection.3.7.2}{牛顿方法讲解}{subsection.3.7}% 26
\BOOKMARK [1][-]{section.4}{广义线性模型}{}% 27
\BOOKMARK [2][-]{subsection.4.1}{指数分布族}{section.4}% 28
\BOOKMARK [2][-]{subsection.4.2}{伯努利分布与高斯分布中，GLM各部分的值}{section.4}% 29
\BOOKMARK [2][-]{subsection.4.3}{如何构造广义线性模型}{section.4}% 30
\BOOKMARK [3][-]{subsubsection.4.3.1}{三个假设}{subsection.4.3}% 31
\BOOKMARK [3][-]{subsubsection.4.3.2}{普通的最小二乘法}{subsection.4.3}% 32
\BOOKMARK [3][-]{subsubsection.4.3.3}{逻辑回归}{subsection.4.3}% 33
\BOOKMARK [3][-]{subsubsection.4.3.4}{说明}{subsection.4.3}% 34
\BOOKMARK [2][-]{subsection.4.4}{Softmax\040Regression}{section.4}% 35
\BOOKMARK [3][-]{subsubsection.4.4.1}{关于的说明}{subsection.4.4}% 36
\BOOKMARK [3][-]{subsubsection.4.4.2}{关于T\(y\)的表示法说明}{subsection.4.4}% 37
\BOOKMARK [3][-]{subsubsection.4.4.3}{证明多项式分布属于指数族分布}{subsection.4.4}% 38
\BOOKMARK [3][-]{subsubsection.4.4.4}{使用Softmax进行分类}{subsection.4.4}% 39
\BOOKMARK [3][-]{subsubsection.4.4.5}{参数的拟合方式}{subsection.4.4}% 40
\BOOKMARK [1][-]{section.5}{生成学习算法}{}% 41
\BOOKMARK [2][-]{subsection.5.1}{生成学习算法简介}{section.5}% 42
\BOOKMARK [3][-]{subsubsection.5.1.1}{判别学习算法&生成学习算法}{subsection.5.1}% 43
\BOOKMARK [2][-]{subsection.5.2}{高斯判别分析}{section.5}% 44
\BOOKMARK [3][-]{subsubsection.5.2.1}{多重正态分布简介}{subsection.5.2}% 45
\BOOKMARK [3][-]{subsubsection.5.2.2}{高斯判别分析模型}{subsection.5.2}% 46
\BOOKMARK [3][-]{subsubsection.5.2.3}{高斯判别分析&逻辑回归}{subsection.5.2}% 47
\BOOKMARK [2][-]{subsection.5.3}{朴素贝叶斯}{section.5}% 48
\BOOKMARK [2][-]{subsection.5.4}{拉普拉斯平滑}{section.5}% 49
\BOOKMARK [3][-]{subsubsection.5.4.1}{背景介绍}{subsection.5.4}% 50
\BOOKMARK [3][-]{subsubsection.5.4.2}{拉普拉斯平滑介绍}{subsection.5.4}% 51
\BOOKMARK [2][-]{subsection.5.5}{文本分类}{section.5}% 52
\BOOKMARK [1][-]{section.6}{支持向量机}{}% 53
\BOOKMARK [2][-]{subsection.6.1}{逻辑回归的缺陷}{section.6}% 54
\BOOKMARK [2][-]{subsection.6.2}{SVM前言}{section.6}% 55
\BOOKMARK [2][-]{subsection.6.3}{Margin介绍}{section.6}% 56
\BOOKMARK [3][-]{subsubsection.6.3.1}{函数间隔}{subsection.6.3}% 57
\BOOKMARK [3][-]{subsubsection.6.3.2}{几何间隔}{subsection.6.3}% 58
\BOOKMARK [3][-]{subsubsection.6.3.3}{函数间隔&几何间隔}{subsection.6.3}% 59
\BOOKMARK [2][-]{subsection.6.4}{最优间隔分类器}{section.6}% 60
\BOOKMARK [3][-]{subsubsection.6.4.1}{将难以优化的目标转为容易优化的}{subsection.6.4}% 61
\BOOKMARK [1][-]{section.7}{附录}{}% 62
\BOOKMARK [2][-]{subsection.7.1}{概念与定义}{section.7}% 63
\BOOKMARK [3][-]{subsubsection.7.1.1}{各类变量}{subsection.7.1}% 64
\BOOKMARK [3][-]{subsubsection.7.1.2}{充分统计量}{subsection.7.1}% 65
\BOOKMARK [3][-]{subsubsection.7.1.3}{自然参数}{subsection.7.1}% 66
\BOOKMARK [3][-]{subsubsection.7.1.4}{先验概率 & 后验概率}{subsection.7.1}% 67
\BOOKMARK [3][-]{subsubsection.7.1.5}{似然函数}{subsection.7.1}% 68
\BOOKMARK [3][-]{subsubsection.7.1.6}{参数学习算法 & 非参数学习算法}{subsection.7.1}% 69
\BOOKMARK [3][-]{subsubsection.7.1.7}{Hessian矩阵}{subsection.7.1}% 70
\BOOKMARK [2][-]{subsection.7.2}{中英对照表}{section.7}% 71
\BOOKMARK [2][-]{subsection.7.3}{思考}{section.7}% 72
\BOOKMARK [1][-]{section.8}{说明}{}% 73
