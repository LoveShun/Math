\subsection{核}
为方便后续内容的理解，我先大体讲一下下面的内容： \\

在我们碰到的问题中，有些在低维下不是线性可分的，但是其在高维下却是线性可分的，于是，我们可以将低维下的特征映射到高维中，然后再高维下进行分割，这里的从低维映射到高维就是的方法就是我们后面会提到的$\phi(x)$；但是，这又会带来一个问题，那就是，映射到高维后，如果仅仅制作映射而不做其他优化，我们计算的时间复杂度就上去了，于是我们就想办法来对计算方法进行改善，这个改善的方法就是使用核函数！通过使用核函数，我们并不需要知道我们从低维映射到高维到底是如何映射的（即我们并不显示地知道$\phi(x)$长什么样，我们只知道通过核函数可以达到与通过$\phi(x)$映射后一样的结果）。 \\

简而言之，引出$\phi(\cdot)$是为了说明低维不可分割的数据在高维可能可以分割；再举一堆例子说明通过得到$\phi(\cdot)$后再求$\phi(\cdot)$内积的方式计算量太大，不可取，有更好的计算方式，那就是使用核函数$K(x, z)$\\

按我个人的经验，吴恩达的视频在你还搞不明白核函数的内容时都听不懂他在讲什么，但是在搞明白后就觉得讲得很清晰（也不排除我没认真听......），可以先参考下下面的这篇博文：\url{http://blog.pluskid.org/?p=685} \\

下面开始正文

\subsubsection{特征映射与核函数引入}
\begin{enumerate}
	\item 在前面的线性回归中，我们提到，为了得到更好的拟合结果，我们可以添加高阶项如$x^2, x^3$等来优化拟合结果。为了区分远有的$x$，以及我们添加的$x^2, x^3, ...$，我们将原有的$x$称为问题的{\color{blue}{属性}}，将新添加的$x^2, x^3,...$以及$x$一起称为问题的{\color{blue}{特征}}。从$x \to \left[\begin{matrix}x \\ x^2 \\ x^3\end{matrix}\right]$称为特征映射。记为$\phi$，如本例子中：$\phi(x) = \left[\begin{matrix}x \\ x^2 \\ x^3\end{matrix}\right]$

	\item 在前面的算法中，我们可以将内积$\langle x, z \rangle$改为$\langle \phi(x), \phi(x) \rangle$，我们将其定义为高斯函数：
	\begin{align}
		K(x, z) = \phi(x)^T \phi(z)
	\end{align}

	\item 但是，在映射后$\phi(x)^T \phi(z)$的计算量太大，比如
	\begin{enumerate}
		\item 我们先定义一个核函数$K(x, z)=(x^T z)^2$，然后找出其特征映射$\phi(x)$，以此说明若要显式地写出$\phi(x)$的式子与只知道核函数不使用显式的$\phi(x)$两者间的计算量差异。
		\item 对$K(x, z)$进一步计算
		\begin{align}
			K(x, z) &= \left(\sum_{i=1}^{n}x_iz_i \sum_{j=1}^{n}x_jz_j \right) \\
			&= \sum_{i=1}^{n} \sum_{j=1}^{n} x_i x_j z_i z_j \\
			&= \sum_{i,j=1}^{n}(x_i x_j) (z_i z_j)
		\end{align}
		如上，以$n=3$为例，若要将$K(x, z)$表示成$\phi(x)^T\phi(z)$的形式:
		\begin{align}
			\phi(x) = \left[\begin{matrix}x_1 x_1 \\ x_1 x_2 \\ x_1 x_3 \\ x_2 x_1 \\ x_2 x_2 \\ x_2 x_3 \\ x_3 x_1 \\ x_3 x_2 \\ x_3 x_3 \\\end{matrix}\right]
		\end{align}
		$\phi(z)$同理。显然，在上面的$\phi(x)$中，若要讲$x \to \phi(x)$，我们需要进行9次的计算；
		\item 但是，若我们不管$\phi(x)$应如何表示，直接先计算$x^Tz$后再取平方，$n=3$时只需要计算3次。
		\item 实际上，此处的$K(x, z) = (x^T z)^2$ 是我们后面会讲到的多项式核$K(x,z)=\left(\langle x, z \rangle +R \right)^d = \left(x^Tz +R\right)^d$的一种。虽然核函数$K(x,z)$可以很容易地表示出来，但是其特征映射$\phi(x)$却不见得容易表示。
	\end{enumerate}

	\item 所以，虽然前面使用将$x$映射到高维$\phi(x)$来说明这样做可以将低维无法线性分隔的点分隔开，但是在实际计算中，我们并不会去计算$\phi(x)$应如何表示，相当于我们实际上使用更容易计算的核函数$K(x, z)$来达到特征映射$\phi(x)$将低维映射到高维的效果

	\item 我们会做的是找到一个核函数，证明它确实对应着存在一个特征映射$\phi(x)$可以实现将数据在高维特征下分割开。

	\item 注: 下面说明下\url{http://blog.pluskid.org/?p=685}中的例子应如何理解
	\begin{enumerate}
		\item 例子中说明提到：$\phi(\cdot)$的目的是使得向量$x_1=(\eta_1, \eta_2)^T, x_2=(\xi_1, \xi_2)^T$在经过$\phi(\cdot)$映射后再求内积的结果为：
		\begin{align}
			\langle \phi(x_1), \phi(x_2) \rangle = \eta_1\xi_1 + \eta_1^2\xi_1^2 + \eta_2\xi_2 + \eta_2^2\xi_2^2 + \eta_1\eta_2\xi_1\xi_2
		\end{align}
		\item 但是，若我们不知道$\phi(\cdot)$是何种形式，直接通过某个核函数$K(x_1, x_2)$也可以得到同样的结果：
		\begin{align}
			\left( \langle x_1, x_2 \rangle +1 \right)^2 &= \langle x_1, x_2 \rangle ^2 + 2 \langle x_1, x_2 \rangle + 1 \\
			&= (x_1^T x_2)^2 + 2x_1^T x_2 + 1 \\
			&=\left\{ \left[\begin{matrix}\eta_1 & \eta_2\end{matrix}\right]\left[\begin{matrix}\xi_1 \\ \xi_2\end{matrix}\right] \right\}^2 + 2\left[\begin{matrix}\eta_1 & \eta_2\end{matrix}\right]\left[\begin{matrix}\xi_1 \\ \xi_2\end{matrix}\right] + 1 \\
			&= (\eta_1\xi_1+\eta_2\xi_2)^2 + 2(\eta_1\xi_1+\eta_2\xi_2) + 1 \\
			&= \eta_1^2\xi_1^2 + \eta_2^2\xi_2^2 + 2\eta_1\eta_2\xi_1\xi_2 + 2\eta_1\xi_1 + 2\eta_2\xi_2 + 1 \\
		\end{align}
		与前面的式子相比，只要对某些维度进行线性缩放即可，说明两者可以达到同样的效果。
		\item 但是，在计算量上，用第一种方法我们需要先找到其对应的$\phi(\cdot)$（若$\phi(x_1, x-2)=(\sqrt{2}x_1, x_1^2, \sqrt{2}x_2, x_2^2, \sqrt{2}x_1x_2, 1)^T$则可得到与$\left( \langle x_1, x_2 \rangle +1 \right)^2$一样的结果），然后再计算$\phi(\cdot)$的内积。显然，通过$\phi(\cdot)$的方式计算量太大，但是使用核函数计算的方式计算量就小多了。这就是该例子的目的
	\end{enumerate}

	\item 对$\phi(\cdot)$的一个直观但可能不是太准确的描述：对于$\phi(x)$和$\phi(z)$，如果两者比较相近，则其核函数$K(x,z)=\phi(x)^T\phi(z)$值较大；若干两者相差较远，则其结果值较小。所以我们可以将核函数$K(x,z)$看成是对$x,z$有多相近的一种表示。
\end{enumerate}

\subsubsection{常用核函数介绍}
\begin{enumerate}
	\item 多项式核
	\begin{align}
		K(x_1, x_2) = \left(\langle x_1, x_2 \rangle + R\right)^d
	\end{align}
	\item 高斯核
	\begin{align}
		K(x_1, x_2) = e^{-\frac{\|x_1 - x_2\|^2}{2\sigma^2}}
	\end{align}
	高斯核会将原始空间映射到无穷维空间。如果$\sigma$选得很大，高次特征上得权重就衰减得非常快，类似于一个低维的子空间；若$\sigma$选得很小，则可将任意的数据映射为线性可分，不过这可能会带来严重的过拟合问题。\\
	通过调整参数$\sigma$，高斯核具有很高的灵活性，它也是使用最广泛的核函数之一。
	\item 线性核
	\begin{align}
		K(x_1, x_2) = \langle x_1, x_2 \rangle
	\end{align}
	这实际上就是原始空间上的内积。
\end{enumerate}

\subsubsection{核函数需要满足的条件}
\begin{enumerate}
	\item 核矩阵
	对于一个有$\{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}$个数据点的数据集，我们定义核矩阵中的某一项$K_{ij}$为:
	\begin{align}
		K_{ij} = K(x^{(i)}, x^{(j)})
	\end{align}
	核矩阵是一个$m\times m$矩阵
	\item 如果$K$是一个满足条件的（合法的）核函数，则
	\begin{align}
		K_{ij} = K(x^{(i)}, x^{(j)}) = \phi(x^{(i)})^T\phi(x^{(j)}) = \phi(x^{(j)})^T\phi(x^{(i)}) = K(x^{(j)}, x^{(i)}) = K_{ji}
	\end{align}
	所以说，一个合法的核函数要求其核矩阵是对称的。
	\item 用$\phi_k(x)$表示向量$\phi(x)$的第$k$项
	\begin{align}
		z^TKz &= \sum_{i}\sum_{j}z_j K_{ij} z_j \\
		&= \sum_{i}\sum_{j} z_i \phi(x^{(i)})^T \phi(x^{(j)}) z_j \\
		&= \sum_{i}\sum_{j}z_i \sum_{k}\phi_k(x^{(i)})\phi_k(x^{(j)})z_j \\
		&= \sum_{k}\sum_{i}\sum_{j}z_i \phi_k(x^{(i)}) \phi_k(x^{(j)}) z_j \\
		&= \sum_{k} \left(\sum_{i}z_i \phi_k(x^{(i)}) \right)^2 \\
		& \geq 0
	\end{align}
	所以，一个合法的核函数要求其核矩阵是半正定矩阵
	\item 综上，如果一个核函数是合法的，那么我们要求其对应的核矩阵应是一个对称半正定矩阵。我们将其称为Mercer条件，具体表述如下：
	\item Mercer条件
	略
	\item 虽然我们对核函数的推导、引入都是在SVM中进行，但是核函数并不仅仅能用在SVM中，在我们前面讲到的线性回归，逻辑回归中，若我们将式子改写成内积的形式，然后再讲内积替换为核函数的方法，就能够在其他算法中使用核函数了。
\end{enumerate}

