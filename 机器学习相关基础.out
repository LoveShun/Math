\BOOKMARK [0][-]{part.1}{I 机器学习基础知识}{}% 1
\BOOKMARK [1][-]{section.1}{线性回归}{part.1}% 2
\BOOKMARK [2][-]{subsection.1.1}{假设函数\(Hypothesis Function\)}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.2}{最小二乘法}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.3}{梯度下降}{section.1}% 5
\BOOKMARK [3][-]{subsubsection.1.3.1}{批梯度下降\(BGD\)}{subsection.1.3}% 6
\BOOKMARK [3][-]{subsubsection.1.3.2}{随机梯度下降\(SGD\)}{subsection.1.3}% 7
\BOOKMARK [3][-]{subsubsection.1.3.3}{迷你批梯度下降\(mini-batch GD\)}{subsection.1.3}% 8
\BOOKMARK [2][-]{subsection.1.4}{线性代数知识}{section.1}% 9
\BOOKMARK [2][-]{subsection.1.5}{梯度下降过程的矩阵表达}{section.1}% 10
\BOOKMARK [2][-]{subsection.1.6}{线性回归中使用最小二乘法的合理性解释}{section.1}% 11
\BOOKMARK [3][-]{subsubsection.1.6.1}{证明过程}{subsection.1.6}% 12
\BOOKMARK [3][-]{subsubsection.1.6.2}{说明}{subsection.1.6}% 13
\BOOKMARK [1][-]{section.2}{局部加权回归\(LWR：Local Weight Regression\)}{part.1}% 14
\BOOKMARK [2][-]{subsection.2.1}{概念}{section.2}% 15
\BOOKMARK [2][-]{subsection.2.2}{计算方法}{section.2}% 16
\BOOKMARK [2][-]{subsection.2.3}{其他注意事项}{section.2}% 17
\BOOKMARK [1][-]{section.3}{逻辑回归}{part.1}% 18
\BOOKMARK [2][-]{subsection.3.1}{sigmoid\040Function}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.2}{假设函数h\(x\)\(Hypothesis Function\)}{section.3}% 20
\BOOKMARK [2][-]{subsection.3.3}{推导过程}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.4}{逻辑回归&线性回归}{section.3}% 22
\BOOKMARK [2][-]{subsection.3.5}{0-1分布&逻辑回归}{section.3}% 23
\BOOKMARK [2][-]{subsection.3.6}{感知器算法}{section.3}% 24
\BOOKMARK [2][-]{subsection.3.7}{牛顿方法}{section.3}% 25
\BOOKMARK [3][-]{subsubsection.3.7.1}{牛顿方法的思路}{subsection.3.7}% 26
\BOOKMARK [3][-]{subsubsection.3.7.2}{牛顿方法讲解}{subsection.3.7}% 27
\BOOKMARK [1][-]{section.4}{广义线性模型}{part.1}% 28
\BOOKMARK [2][-]{subsection.4.1}{指数分布族}{section.4}% 29
\BOOKMARK [2][-]{subsection.4.2}{伯努利分布与高斯分布中，GLM各部分的值}{section.4}% 30
\BOOKMARK [2][-]{subsection.4.3}{如何构造广义线性模型}{section.4}% 31
\BOOKMARK [3][-]{subsubsection.4.3.1}{三个假设}{subsection.4.3}% 32
\BOOKMARK [3][-]{subsubsection.4.3.2}{普通的最小二乘法}{subsection.4.3}% 33
\BOOKMARK [3][-]{subsubsection.4.3.3}{逻辑回归}{subsection.4.3}% 34
\BOOKMARK [3][-]{subsubsection.4.3.4}{说明}{subsection.4.3}% 35
\BOOKMARK [2][-]{subsection.4.4}{Softmax\040Regression}{section.4}% 36
\BOOKMARK [3][-]{subsubsection.4.4.1}{关于的说明}{subsection.4.4}% 37
\BOOKMARK [3][-]{subsubsection.4.4.2}{关于T\(y\)的表示法说明}{subsection.4.4}% 38
\BOOKMARK [3][-]{subsubsection.4.4.3}{证明多项式分布属于指数族分布}{subsection.4.4}% 39
\BOOKMARK [3][-]{subsubsection.4.4.4}{使用Softmax进行分类}{subsection.4.4}% 40
\BOOKMARK [3][-]{subsubsection.4.4.5}{参数的拟合方式}{subsection.4.4}% 41
\BOOKMARK [1][-]{section.5}{生成学习算法}{part.1}% 42
\BOOKMARK [2][-]{subsection.5.1}{生成学习算法简介}{section.5}% 43
\BOOKMARK [3][-]{subsubsection.5.1.1}{判别学习算法&生成学习算法}{subsection.5.1}% 44
\BOOKMARK [2][-]{subsection.5.2}{高斯判别分析}{section.5}% 45
\BOOKMARK [3][-]{subsubsection.5.2.1}{多重正态分布简介}{subsection.5.2}% 46
\BOOKMARK [3][-]{subsubsection.5.2.2}{高斯判别分析模型}{subsection.5.2}% 47
\BOOKMARK [3][-]{subsubsection.5.2.3}{高斯判别分析&逻辑回归}{subsection.5.2}% 48
\BOOKMARK [2][-]{subsection.5.3}{朴素贝叶斯}{section.5}% 49
\BOOKMARK [2][-]{subsection.5.4}{拉普拉斯平滑}{section.5}% 50
\BOOKMARK [3][-]{subsubsection.5.4.1}{背景介绍}{subsection.5.4}% 51
\BOOKMARK [3][-]{subsubsection.5.4.2}{拉普拉斯平滑介绍}{subsection.5.4}% 52
\BOOKMARK [2][-]{subsection.5.5}{文本分类}{section.5}% 53
\BOOKMARK [1][-]{section.6}{支持向量机}{part.1}% 54
\BOOKMARK [2][-]{subsection.6.1}{逻辑回归的缺陷}{section.6}% 55
\BOOKMARK [2][-]{subsection.6.2}{SVM前言}{section.6}% 56
\BOOKMARK [2][-]{subsection.6.3}{Margin介绍}{section.6}% 57
\BOOKMARK [3][-]{subsubsection.6.3.1}{函数间隔}{subsection.6.3}% 58
\BOOKMARK [3][-]{subsubsection.6.3.2}{几何间隔}{subsection.6.3}% 59
\BOOKMARK [3][-]{subsubsection.6.3.3}{函数间隔&几何间隔}{subsection.6.3}% 60
\BOOKMARK [3][-]{subsubsection.6.3.4}{对SVM设计函数间隔、几何间隔原因的思考}{subsection.6.3}% 61
\BOOKMARK [2][-]{subsection.6.4}{最优间隔分类器-优化目标}{section.6}% 62
\BOOKMARK [3][-]{subsubsection.6.4.1}{将难以优化的目标转为容易优化的}{subsection.6.4}% 63
\BOOKMARK [2][-]{subsection.6.5}{拉格朗日对偶规划}{section.6}% 64
\BOOKMARK [3][-]{subsubsection.6.5.1}{拉格朗日乘数法}{subsection.6.5}% 65
\BOOKMARK [3][-]{subsubsection.6.5.2}{拉格朗日对偶规划}{subsection.6.5}% 66
\BOOKMARK [2][-]{subsection.6.6}{最优间隔分类器-优化方法}{section.6}% 67
\BOOKMARK [2][-]{subsection.6.7}{核}{section.6}% 68
\BOOKMARK [1][-]{section.7}{附录}{part.1}% 69
\BOOKMARK [2][-]{subsection.7.1}{概念与定义}{section.7}% 70
\BOOKMARK [3][-]{subsubsection.7.1.1}{各类变量}{subsection.7.1}% 71
\BOOKMARK [3][-]{subsubsection.7.1.2}{充分统计量}{subsection.7.1}% 72
\BOOKMARK [3][-]{subsubsection.7.1.3}{自然参数}{subsection.7.1}% 73
\BOOKMARK [3][-]{subsubsection.7.1.4}{先验概率 & 后验概率}{subsection.7.1}% 74
\BOOKMARK [3][-]{subsubsection.7.1.5}{似然函数}{subsection.7.1}% 75
\BOOKMARK [3][-]{subsubsection.7.1.6}{参数学习算法 & 非参数学习算法}{subsection.7.1}% 76
\BOOKMARK [3][-]{subsubsection.7.1.7}{Hessian矩阵}{subsection.7.1}% 77
\BOOKMARK [2][-]{subsection.7.2}{中英对照表}{section.7}% 78
\BOOKMARK [2][-]{subsection.7.3}{思考}{section.7}% 79
\BOOKMARK [0][-]{part.2}{II Coursera机器学习}{}% 80
\BOOKMARK [1][-]{section.8}{线性回归\(Linear Regression\)}{part.2}% 81
\BOOKMARK [2][-]{subsection.8.1}{各向量形式}{section.8}% 82
\BOOKMARK [2][-]{subsection.8.2}{批梯度下降}{section.8}% 83
\BOOKMARK [3][-]{subsubsection.8.2.1}{各矩阵形式}{subsection.8.2}% 84
\BOOKMARK [3][-]{subsubsection.8.2.2}{Cost\040Function}{subsection.8.2}% 85
\BOOKMARK [3][-]{subsubsection.8.2.3}{偏导数J\(\)j计算}{subsection.8.2}% 86
\BOOKMARK [3][-]{subsubsection.8.2.4}{梯度下降迭代方式}{subsection.8.2}% 87
\BOOKMARK [2][-]{subsection.8.3}{Feature\040Normalization}{section.8}% 88
\BOOKMARK [2][-]{subsection.8.4}{公式法求解（Normal Equation）}{section.8}% 89
\BOOKMARK [1][-]{section.9}{逻辑回归\(Logistic Regression\)}{part.2}% 90
\BOOKMARK [2][-]{subsection.9.1}{当只有2个类别时，使用1个分类器}{section.9}% 91
\BOOKMARK [3][-]{subsubsection.9.1.1}{sigmoid函数}{subsection.9.1}% 92
\BOOKMARK [3][-]{subsubsection.9.1.2}{预测函数}{subsection.9.1}% 93
\BOOKMARK [3][-]{subsubsection.9.1.3}{Cost\040Function}{subsection.9.1}% 94
\BOOKMARK [3][-]{subsubsection.9.1.4}{偏导数J\(\)j}{subsection.9.1}% 95
\BOOKMARK [3][-]{subsubsection.9.1.5}{梯度下降迭代算法}{subsection.9.1}% 96
\BOOKMARK [2][-]{subsection.9.2}{当只有k个类别时，使用k个分类器}{section.9}% 97
\BOOKMARK [1][-]{section.10}{Regularization}{part.2}% 98
\BOOKMARK [2][-]{subsection.10.1}{线性回归}{section.10}% 99
\BOOKMARK [3][-]{subsubsection.10.1.1}{数值计算方式}{subsection.10.1}% 100
\BOOKMARK [3][-]{subsubsection.10.1.2}{矩阵计算方式}{subsection.10.1}% 101
\BOOKMARK [2][-]{subsection.10.2}{逻辑回归}{section.10}% 102
\BOOKMARK [3][-]{subsubsection.10.2.1}{数值计算方式}{subsection.10.2}% 103
\BOOKMARK [3][-]{subsubsection.10.2.2}{矩阵计算方式}{subsection.10.2}% 104
\BOOKMARK [2][-]{subsection.10.3}{注意}{section.10}% 105
\BOOKMARK [1][-]{section.11}{神经网络–前向算法}{part.2}% 106
\BOOKMARK [2][-]{subsection.11.1}{神经网络示意图–前向算法}{section.11}% 107
\BOOKMARK [2][-]{subsection.11.2}{神经网络–前向算法}{section.11}% 108
\BOOKMARK [3][-]{subsubsection.11.2.1}{X、、、z、a}{subsection.11.2}% 109
\BOOKMARK [3][-]{subsubsection.11.2.2}{y}{subsection.11.2}% 110
\BOOKMARK [1][-]{section.12}{神经网络–后向算法}{part.2}% 111
\BOOKMARK [2][-]{subsection.12.1}{神经网络示意图–后向算法}{section.12}% 112
\BOOKMARK [2][-]{subsection.12.2}{神经网络–后向算法}{section.12}% 113
\BOOKMARK [3][-]{subsubsection.12.2.1}{输出层结果：aL}{subsection.12.2}% 114
\BOOKMARK [3][-]{subsubsection.12.2.2}{格式化后的Y}{subsection.12.2}% 115
\BOOKMARK [3][-]{subsubsection.12.2.3}{L}{subsection.12.2}% 116
\BOOKMARK [3][-]{subsubsection.12.2.4}{L-1}{subsection.12.2}% 117
\BOOKMARK [3][-]{subsubsection.12.2.5}{l\(2<=l<=L-2\)}{subsection.12.2}% 118
\BOOKMARK [3][-]{subsubsection.12.2.6}{l（用迭代的方式计算）}{subsection.12.2}% 119
\BOOKMARK [3][-]{subsubsection.12.2.7}{Dij\(l\)}{subsection.12.2}% 120
\BOOKMARK [3][-]{subsubsection.12.2.8}{J\(\)ij\(l\)}{subsection.12.2}% 121
\BOOKMARK [3][-]{subsubsection.12.2.9}{l 与 l的区别与联系}{subsection.12.2}% 122
\BOOKMARK [1][-]{section.13}{调试技巧}{part.2}% 123
\BOOKMARK [2][-]{subsection.13.1}{Error\040Analysis}{section.13}% 124
\BOOKMARK [2][-]{subsection.13.2}{Error\040Metrics\040for\040Skewed\040Classes}{section.13}% 125
\BOOKMARK [2][-]{subsection.13.3}{如何评价Precision与Recall}{section.13}% 126
\BOOKMARK [2][-]{subsection.13.4}{拟合效果不好时的解决方法指导}{section.13}% 127
\BOOKMARK [2][-]{subsection.13.5}{不同神经网络的优缺点}{section.13}% 128
\BOOKMARK [2][-]{subsection.13.6}{绘制Learning Curve}{section.13}% 129
\BOOKMARK [1][-]{section.14}{SVM}{part.2}% 130
\BOOKMARK [2][-]{subsection.14.1}{Cost\040Function}{section.14}% 131
\BOOKMARK [2][-]{subsection.14.2}{Gaussian\040Kernel}{section.14}% 132
\BOOKMARK [2][-]{subsection.14.3}{SVM中，C与2对欠拟合或过拟合的影响}{section.14}% 133
\BOOKMARK [2][-]{subsection.14.4}{如何选项使用Logistic Regression还是 SVM}{section.14}% 134
\BOOKMARK [0][-]{part.3}{III 概率统计基础知识}{}% 135
\BOOKMARK [1][-]{section.15}{随机事件和概率}{part.3}% 136
\BOOKMARK [2][-]{subsection.15.1}{基本概念及公式}{section.15}% 137
\BOOKMARK [1][-]{section.16}{常用分布}{part.3}% 138
\BOOKMARK [2][-]{subsection.16.1}{离散型}{section.16}% 139
\BOOKMARK [2][-]{subsection.16.2}{连续型}{section.16}% 140
\BOOKMARK [2][-]{subsection.16.3}{泊松定理}{section.16}% 141
\BOOKMARK [1][-]{section.17}{多维随机变量及其分布}{part.3}% 142
\BOOKMARK [2][-]{subsection.17.1}{二维随机变量及其分布}{section.17}% 143
\BOOKMARK [3][-]{subsubsection.17.1.1}{连续型}{subsection.17.1}% 144
\BOOKMARK [3][-]{subsubsection.17.1.2}{离散型}{subsection.17.1}% 145
\BOOKMARK [2][-]{subsection.17.2}{概率密度}{section.17}% 146
\BOOKMARK [3][-]{subsubsection.17.2.1}{连续型}{subsection.17.2}% 147
\BOOKMARK [3][-]{subsubsection.17.2.2}{离散型}{subsection.17.2}% 148
\BOOKMARK [2][-]{subsection.17.3}{分布函数与概率密度的性质}{section.17}% 149
\BOOKMARK [3][-]{subsubsection.17.3.1}{F\(x,y\)的性质}{subsection.17.3}% 150
\BOOKMARK [3][-]{subsubsection.17.3.2}{P\(X=xi, Y=yj\)=pij的性质}{subsection.17.3}% 151
\BOOKMARK [3][-]{subsubsection.17.3.3}{f\(x,y\)的性质}{subsection.17.3}% 152
\BOOKMARK [2][-]{subsection.17.4}{随机变量的独立性}{section.17}% 153
\BOOKMARK [3][-]{subsubsection.17.4.1}{随机变量X与Yred相互独立的red充要条件}{subsection.17.4}% 154
\BOOKMARK [2][-]{subsection.17.5}{二维均匀分布&二维正态分布}{section.17}% 155
\BOOKMARK [2][-]{subsection.17.6}{两个随机变量函数Z=g\(X,Y\)的分布}{section.17}% 156
\BOOKMARK [1][-]{section.18}{随机变量的数字特征}{part.3}% 157
\BOOKMARK [2][-]{subsection.18.1}{随机变量的数学期望和方差}{section.18}% 158
\BOOKMARK [3][-]{subsubsection.18.1.1}{期望}{subsection.18.1}% 159
\BOOKMARK [3][-]{subsubsection.18.1.2}{方差}{subsection.18.1}% 160
\BOOKMARK [3][-]{subsubsection.18.1.3}{常用随机变量的数学期望和方差}{subsection.18.1}% 161
\BOOKMARK [2][-]{subsection.18.2}{矩、协方差、相关系数}{section.18}% 162
\BOOKMARK [1][-]{section.19}{大数定律与中心极限定律}{part.3}% 163
\BOOKMARK [1][-]{section.20}{数理统计的基本概念}{part.3}% 164
\BOOKMARK [2][-]{subsection.20.1}{总体&样本}{section.20}% 165
\BOOKMARK [2][-]{subsection.20.2}{统计量}{section.20}% 166
\BOOKMARK [2][-]{subsection.20.3}{样本的数字特征}{section.20}% 167
\BOOKMARK [2][-]{subsection.20.4}{样本的数字特征的性质}{section.20}% 168
\BOOKMARK [2][-]{subsection.20.5}{常用统计抽样分布}{section.20}% 169
\BOOKMARK [3][-]{subsubsection.20.5.1}{2分布}{subsection.20.5}% 170
\BOOKMARK [3][-]{subsubsection.20.5.2}{t分布}{subsection.20.5}% 171
\BOOKMARK [3][-]{subsubsection.20.5.3}{F分布}{subsection.20.5}% 172
\BOOKMARK [2][-]{subsection.20.6}{正态总体的抽样分布}{section.20}% 173
\BOOKMARK [3][-]{subsubsection.20.6.1}{一个正态总体的抽样分布}{subsection.20.6}% 174
\BOOKMARK [3][-]{subsubsection.20.6.2}{两个正态总体的抽样分布}{subsection.20.6}% 175
\BOOKMARK [1][-]{section.21}{参数估计}{part.3}% 176
\BOOKMARK [2][-]{subsection.21.1}{点估计}{section.21}% 177
\BOOKMARK [2][-]{subsection.21.2}{矩估计法}{section.21}% 178
\BOOKMARK [2][-]{subsection.21.3}{最大似然估计法}{section.21}% 179
\BOOKMARK [2][-]{subsection.21.4}{区间估计}{section.21}% 180
\BOOKMARK [1][-]{section.22}{假设检验}{part.3}% 181
\BOOKMARK [2][-]{subsection.22.1}{假设检验}{section.22}% 182
\BOOKMARK [2][-]{subsection.22.2}{正态总体参数的假设检验}{section.22}% 183
\BOOKMARK [1][-]{section.23}{中心极限定理}{part.3}% 184
\BOOKMARK [2][-]{subsection.23.1}{待添加}{section.23}% 185
\BOOKMARK [1][-]{section.24}{常用公式}{part.3}% 186
