\section{逻辑回归}
\subsection{sigmoid Function}
\begin{enumerate}
	\item sigmoid Function
	\begin{equation}
		g(z) = \frac{1}{1+e^{-z}}
	\end{equation}
	将sigmoid Function作用在线性回归的$h_\theta(x)$上，将线性回归的结果压缩到$(0,1)$.
	\item sigmoid Function的性质
	\begin{align}
		g^{'}{(x)} &= \frac{d}{dz}\frac{1}{1+e^{-z}} \\
		&= -1 \cdot \frac{1}{(1+e^{-z})^2} \cdot e^{-z} \cdot (-1) \\
		&= \frac{1+e^{-z} - 1}{(1+e^{-z})^2} \\
		&= \frac{1}{1+e^{-z}} \cdot \left( 1- \frac{1}{1+e^{-z}} \right) \\
		&= g(z)\left[1-g(z)\right]
	\end{align}
\end{enumerate}

\subsection{假设函数$h_\theta(x)$(Hypothesis Function)}
\begin{enumerate}
	\item 在逻辑回归中，Hypothesis Function定义为：在给定的数据集$x$的条件下，得到$y=1$的概率，即：
	\begin{equation}
		h_\theta(x) = P(y=1|x; \theta)
	\end{equation}
	在上式中，$P(y=1|x)$表示在$x$条件下，$y=1$的概率；$\theta$表示该式子是关于$\theta$的函数 \\
	如前面据说，逻辑回归是将线性回归作用在sigmoid Function中得到的，故，$h_\theta(x)$还可以表示为以下形式
	\begin{equation}
		h_\theta(x) = g(\Theta^Tx)
	\end{equation}
	其中，$\Theta$表示由各个$\theta_j$组成的向量；$x$表示数据集中的某组数据（所有数据集用$X$表示）

	\item 对二元分类来说，我们可以得到以下式子
	\begin{align}
		P(y=1|x;\theta) &= h_\theta(x) \\
		P(y=0|x;\theta) &= 1- h_\theta(x)
	\end{align}
	以上两个式子可以改写成一个式子，以便做后续的计算，如下：
	\begin{equation}
		p(y|x;\theta) = \left( h_\theta(x) \right)^y \left( 1-h_\theta(x) \right)^{1-y}
	\end{equation}
	在$y=0$时，为$1- h_\theta(x)$，在$y=1$时，为$h_\theta(x)$
\end{enumerate}

\subsection{推导过程}
\begin{enumerate}
	\item 回想一下线性回归的推导方式，是使用最小二乘法计算误差（不同的梯度下降对应不同的误差），然后使用最小二乘法进行迭代，得到使误差最小的$\theta$，使用此$\theta$进行预测

	\item 但逻辑回归与其不同，下面我们先讲下逻辑回归中使用的推导方法，然后再说明下为什么其与线性回归不同
	\begin{enumerate}
		\item 用概率论的语言讲，我们的逻辑回归其实是讨论在多维（离散??or 连续??）随机变量$X$中，$Y=g(\Theta^TX)$发生的概率。其中，$x_i$的维数就是该多维随机变量的维数，$g(\theta^Tx_i)$得到的结果就是$y=1$的概率。

		\item 对于此种概率问题，我们要如何评价算法的好坏呢？下面我们插播下评价此类问题算法的好坏的方法
		\begin{enumerate}
			\item 如果我们和线性回归类似，通过$J(\theta) = \frac{1}{2} \sum_{}^{} \left[h_\theta(x^{(i)}) - y^{(i)}\right]^2$，再求使$J(\theta)$最小的$\theta$是否可以呢？\\
			理论上这似乎是可以的，那我们来计算一下
			\begin{align}
				\frac{\partial} {\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{1}{2} \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right]^2 \\
			      &= 2 * \frac{1}{2} \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right] \frac{\partial}{\partial\theta_j}\left[ h_\theta(x^{(i)}) - y^{(i)} \right] \\
			      &= \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right]\frac{\partial}{\partial\theta_j}\left[ \frac{1}{1+e^{\theta_0x_0^{(i)} +  \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + ... + \theta_nx_n^{(i)}} } - y^{(i)} \right] \\
			      &= \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right] \cdot \frac{-1\cdot e^{(\theta_0x_0^{(i)}+ \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + ... + \theta_nx_n^{(i)})}\cdot x^{(i)}}{\left[1+e^{\theta_0x_0^{(i)} +  \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + ... + \theta_nx_n^{(i)}}\right]^2} \\
			      &= \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right] \cdot \frac{-1\cdot e^{\Theta^Tx^{(i)}}\cdot x^{(i)}}{\left[1 + e^{\Theta^Tx^{(i)}}\right]^2} \\
			      &= \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right] \cdot \frac{-1\cdot e^{h_\theta(x)}\cdot x^{(i)}}{\left[1 + e^{h_\theta(x)}\right]^2}
			\end{align}
			与线性回归简洁的结果$\frac{\partial} {\partial \theta_j} J(\theta) = \sum_{i=1}^m\left[ h_\theta(x^{(i)}) - y^{(i)} \right]x_j^{(i)}$不一样，逻辑回归得到的结果太复杂，因此并不适合使用此方法
			\item 既然最小二乘法不合适，那我们就应寻找其他的方法，经过不段努力，发现使用最大似然估计法是可用的。

			\item 最大似然估计
			\begin{align}
				L(\theta) &= \prod_{i=1}^{m}p(y^{(i)}|x^{(i)}; \theta) \\
				&= \prod_{i=1}^{m}\left( h_\theta(x^{(i)}) \right)^{y^{(i)}} \left( 1-h_\theta(x^{(i)}) \right)^{1-y^{(i)}}
			\end{align}
			我们要做的就是让$L(\theta)$最大化。显然，对于这种指数问题，要求其最值，第一件事就是取其对数，然后才是求导，$=0$，计算
			\begin{align}
				l(\theta) &= \log L(\theta) \\
				&= \sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))
			\end{align}
			\begin{align}
				\frac{\partial}{\partial \theta_j}l(\theta) &= \frac{\partial}{\partial \theta_j} \left[ \sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log(1-h_\theta(x^{(i)})) \right]\\
				&= \sum_{i=1}^{m} \left( y^{(i)}\frac{1}{h_\theta(x^{(i)})} \frac{\partial}{\partial\theta_j}h_\theta(x^{(i)}) - (1-y^{(i)})\frac{1}{1-h_\theta(x^{(i)})}\frac{\partial}{\partial\theta_j}h_\theta(x^{(i)})  \right) \\
				&= \sum_{i=1}^{m} \left( y^{(i)}\frac{1}{g(\Theta^Tx^{(i)})} - (1-y^{(i)})\frac{1}{1-g(\Theta^Tx)} \right) \frac{\partial}{\partial\theta_j}g(\Theta^Tx^{(i)}) \\
				&= \sum_{i=1}^{m} \left( y^{(i)}\frac{1}{g(\Theta^Tx^{(i)})} - (1-y^{(i)})\frac{1}{1-g(\Theta^Tx^{(i)})} \right)g(\Theta^Tx^{(i)})(1-g(\Theta^Tx^{(i)})) \frac{\partial}{\partial\theta_j}\Theta^Tx^{(i)} \\
				&= \sum_{i=1}^{m} \left\{ y^{(i)}\left[1-g(\Theta^Tx^{(i)})\right] - (1-y^{(i)})g(\Theta^Tx^{(i)}) \right\}x_j^{(i)} \\
				&= \sum_{i=1}^{m} \left[ y^{(i)} - h_\theta(x^{(i)}) \right]x_j^{(i)}
			\end{align}
			上面的推导过程中，使用到了$g^{'}{(x)}=g(z)\left[1-g(z)\right]$

			\item 为了使$L(\theta)$最大，使用梯度上升算法，最终结果
			\begin{align}
				\theta_j &:= \theta_j + \alpha \frac{\partial}{\partial \theta_j}l(\theta) \\
				&=  \theta_j + \alpha \sum_{i=1}^{m} \left[ y^{(i)} - h_\theta(x^{(i)}) \right]x_j^{(i)}
			\end{align}
			注意，此处，我们想要让$L(\theta)$最大，所以使用的是梯度上升，所以上式中中间应是加号$+$。注意其中$y^{(i)}$的位置与$h_\theta(x^{(i)})$的位置与之前线性回归的不一致，更改位置后发现，线性回归与逻辑回归的迭代方式是一样的。
			\begin{align}
				\theta_j :=  \theta_j - \alpha \sum_{i=1}^{m} \left[h_\theta(x^{(i)}) - y^{(i)} \right]x_j^{(i)}
			\end{align}
			为什么这二者会殊途同归呢？是不是其有什么内在联系呢？这将会在后续的内容中介绍

		\end{enumerate}
	\end{enumerate}

\end{enumerate}



\subsection{逻辑回归与线性回归的异同}
\begin{enumerate}
	\item 线性回归推导时使用的是梯度下降，逻辑回归推导时使用的是梯度上升
	\item 同样地，对于逻辑回归，同样有批梯度上升、随机梯度上升、迷你梯度上升
	\item 梯度下降时，前面的符号是减号，梯度上升时，前面的符号是加号，这就相当于默认了算出来的梯度是正值，why??或者有其他的原因？
\end{enumerate}






















