\subsection{高斯判别分析}
以下内容中均假设$p(x|y)$服从多重正态分布（高斯分布）

\subsubsection{多重正态分布简介}
\begin{enumerate}
	\item $n$重正态分布可由均值向量$\mu \in {\rm I\!R}^n$和协方差矩阵$\Sigma \in {\rm I\!R}^{n\times n}$确定，记做$N(\mu, \Sigma)$，表示为：
	\begin{equation}
		p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
	\end{equation}
	其中，$|\Sigma|$是协方差矩阵$\Sigma$的行列式；$\Sigma=\mathrm{Cov}(X) = E(XX^T) - E(X)\left[E(X)\right]^T$，$X$的期望为$E(X) = \int_{x}xp(x;\mu,\Sigma)dx = \mu$
\end{enumerate}

\subsubsection{高斯判别分析模型}
\begin{enumerate}
	\item 高斯判别分析模型针对的是连续性随机变量，如果是离散型随机变量请使用后续会讲到的朴素贝叶斯
	\item 我们假设$p(x|y)$服从多远高斯分布，于是
	\begin{align}
		y & \sim B(1, \phi) = Bernoulli(\phi) \\
		x|y=0 &\sim N(\mu_0, \Sigma) \\
		x|y=1 &\sim N(\mu_1, \Sigma)
	\end{align}
	于是，其概率密度为：
	\begin{align}
		p(y) &= \phi^y(1-\phi)^{1-y} \\
		p(x|y=0) &= \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)} \\
		p(x|y=1) &= \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)}
	\end{align}
	在上式中，$\phi, \mu_0, \mu_1, \Sigma$就是我们要求的参数（注意，有2个$\mu$，共享1个$\Sigma$），其对数似然函数为：
	\begin{align}
		l(\phi, \mu_0, \mu_1, \Sigma) &= \log \prod_{i=1}^{m} p\left(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma\right) \\
		&= \log \prod_{i=1}^{m} p\left(x^{(i)}|y^{(i)}; \mu_0, \mu_1, \Sigma\right)p\left(y^{(i)};\phi\right)
	\end{align}
	注意$p(x|y)$中并没有参数$\phi$，$p(y)$中只有参数$\phi$
	\item 求解似然函数后可得参数如下
	\begin{align}
		\phi &= \frac{1}{m} \sum_{i=1}^{m}1\{y^{(i)=1}\} \\
		\mu_0 &= \frac{\sum_{i=1}^{m}1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^{m}1\{y^{(i)}=0\}} \\
		\mu_1 &= \frac{\sum_{i=1}^{m}1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^{m}1\{y^{(i)}=1\}} \\
		\Sigma &= \frac{1}{m}\sum_{i=1}^{m}\left(x^{(i)}-\mu_{y^{(i)}}\right)\left(x^{(i)}-\mu_{y^{(i)}}\right)^T
	\end{align}
	下面解释下以上各参数表达式的意义： \\
	$\phi$：表示所有数据中$y=1$所占的比例； \\
	$\mu_0$: 其分母表示所有数据中$y=0$的数目，分子表示$y=0$对应的$x$的和；$\mu_1$同理 \\
	$\Sigma$: 协方差矩阵，前面已说过，不再赘述
	\item 说明
	\begin{enumerate}
		\item 使用通过高速判别分析得到的模型得到的是两个高斯分布
		\item 这两个高速分布有共同的协方差$\Sigma$，但是其期望$\mu$不一样，若画出其等高线，在图形上表现为两个等高线形状（由$\Sigma$决定）一样，但是其中心（由$\mu$决定）不一样。
	\end{enumerate}
\end{enumerate}

\subsubsection{高斯判别分析\&逻辑回归}
\begin{enumerate}
	\item 如果我们将$\phi, \mu_0, \mu_1, \Sigma$看出$x$的函数，进行整理后最终可得到
	\begin{equation}
		p(y=1|x; \phi, \mu_0, \mu_1, \Sigma) = \frac{1}{1+e^{-\theta^Tx}}
	\end{equation}
	其中，$\theta$是$\phi, \mu_0, \mu_1, \Sigma$的函数
	\item 在我们的证明过程中，我们假设了$p(x|y)$服从高斯分布，相较于逻辑回归，GDA做了更强的假设。
	\item 如果$p(x|y)$服从多重高斯分布，则$p(y|x)$服从逻辑回归；但反，从$p(y|x)$服从逻辑回归无法得到$p(x|y)$服从多重高斯分布。这说明GDA需要更多的条件，其做了更强的假设。
	\item 在两种算法的准确性上：如果$p(x|y)$确实服从或近似服从高斯分布（即我们对其做的假设正确），那么GDA能够更好地进行预测；但是，若我们的假设错误（即$p(y|y)$实际并不服从高斯分布，可能是泊松分布或其他），此时，逻辑回归能得到更好的效果
	\item 在数据集$m$很大时，GDA可以得到比大部分算法更好的结果（包括逻辑回归）；甚至在数据集较小时，GDA也往往比逻辑回归好
	\item 通常来讲，使用生成学习算法需要的数据更少（因为做了高斯分布的假设，从高斯分布继承了很多性质），因为大部分情况下数据虽然不是精确的高斯分布，但近似服从高斯分布。
	\item 因为逻辑回归的假设更弱（可理解为需要的条件更少），因此其具有更好的鲁棒性（可理解为适应能力更好），即使$p(y|x)$并不符合也不近似符合高斯分布，逻辑回归也能较好地预测。
	\item 使用逻辑回归可能需要比较多的数据样本
\end{enumerate}















