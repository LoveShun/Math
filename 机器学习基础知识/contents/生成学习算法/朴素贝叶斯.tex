\subsection{朴素贝叶斯}
\begin{enumerate}
	\item 在GDA中，随机变量$X$是连续的，当随机变量是离散值时，我们使用朴素贝叶斯来进行预测
	\item 朴素贝叶斯举例介绍：以通过邮箱中的文本预测是否为垃圾邮件为例，$y=1$表示该邮件是垃圾邮件
	\begin{enumerate}
		\item 对每封邮件建立词向量\footnote{此处的词向量比较简单，与文本分析（NLP）中的词向量不一样，也不是One-Hot的形式}，每个词在向量中占据一个位置，若该词存在，则将该位的值设为1，若不存在则设为0，得到如下向量
		\begin{align}
			x = \left[\begin{matrix}1 \\ 0 \\ 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{matrix}\right] \quad
			\begin{matrix}a \\ ad \\ address \\ \vdots \\ buy \\ \vdots \\ zygmurgy \end{matrix}
		\end{align}
		如上所示，该邮件中有a, buy等，没有ad, address, zygmurgy等。
		\footnote{一般来说，建议通过扫描训练集来获取可能出现的词（而不是从字典中获取），这样做一方面可以减小建模时的特征数，节省计算量和存储空间；另一方面还可以避免出现一些字典中没有的词（如CS299，或一些人名等）\\
		另外，还可以将一些无实意、出现频率又比较高的词去除，如a, an, the等}
		\item 若要使用多项式分布来进行预测，按照之前的逻辑回归方式，在词向量维度较多时并不可取（PS：讲义中的$2^{50000}-1$看起来似乎有问题，对$50000$个词建模只需要有$50000+1$个参数，哪来的$2^{50000}-1$?？{\color{red}{待研究...}}）
		\item 为了更方便地进行建模，我们做了一个假设：假设对于给定的$y$，$x_i$（$x_i$指词向量中的第$i$个词）是相互条件独立的，即$p(x_2|y, x_1) = p(x_2|y)$，给定的$x_1$条件并不影响$p(x_2|y)$。此假设称为{\color{blue}{朴素贝叶斯假设}}。由此得到的分类器称为{\color{blue}{朴素贝叶斯分类器}}。
		\item 根据以上假设，我们可以得到以下式子：
		\begin{align}
			p(x_1, \dots, x_n|y) &= p(x_1|y)p(x_2|y, x_1)p(x_3|y, x_1, x_2)\dots p(x_n|y, x_1, x_2, \dots, x_{n-1}) \\
			&= p(x_1|y)p(x_2|y)p(x_3|y)\dots p(x_n|y) \\
			&= \prod_{i=1}^{n}p(x_i|y)
		\end{align}
		\item 与GDA类似，上式由参数$\phi_{i|y=1}=p(x_i=1|y=1)$，$\phi_{i|y=0}=p(x_i=1|y=0)$，$\phi_y=p(y=1)$决定。似然函数可表示为：
		\begin{align}
			L(\phi_y, \phi_{i|y=0}, \phi_{i|y=1}) = \prod_{i=1}^{n}p(x^{(i)}|y^{(i)})
		\end{align}
		\item 通过最大似然估计，可得
		\begin{align}
			\phi_{j|y=1} &= \frac{\sum_{i=1}^{m}1\{x_j^{(i)}=1 \cap y^{(i)}=1\}}{\sum_{i=1}^{m}1\{y^{(i)}=1\}} \\
			\phi_{j|y=0} &= \frac{\sum_{i=1}^{m}1\{x_j^{(i)}=1 \cap y^{(i)}=0\}}{\sum_{i=1}^{m}1\{y^{(i)}=0\}} \\
			\phi_y &= \frac{\sum_{i=1}^{m}1\{y^{(i)}=1\}}{m}
		\end{align}
		其中: \\
		$\cap$表并集，即两个条件同时满足 \\
		$\phi_{j|y=1}$表示出现词$x_j$的垃圾邮件在所有垃圾邮件中所占的比例 \\
		$\phi_{j|y=0}$表示出现词$x_j$的非垃圾邮件在所有非垃圾邮件中所占的比例 \\
		$\phi_y$表示垃圾邮件占所有邮件的比例
		\item 使用贝叶斯公式计算$p(y|x)$
		\begin{align}
			p(y=1|x) &= \frac{p(x|y=1)p(y=1)}{p(x)} \\
			&= \frac{\prod_{i=1}^{n}p(x_i|y=1)p(y=1)}{\left[\prod_{i=1}^{n}p(x_i|y=1)\right]p(y=1)+\left[\prod_{i=1}^{n}p(x_i|y=0)\right]p(y=0)}
		\end{align}
		对$p(y=0|x)$同理，通过比较那个概率更高\footnote{所以，其实$p(x)$并没有必要求，虽然也不难求}来判断是否为垃圾邮件。
		\item 虽然以上过程中只是二分类，但实际上，对于多分类也是以上的套路；
		\item 对于连续型随机变量，也可以对其分段离散化，然后使用朴素贝叶斯进行预测
	\end{enumerate}
\end{enumerate}

\subsection{拉普拉斯平滑}
\subsubsection{背景介绍}
\begin{enumerate}
	\item 在朴素贝叶斯中，若在预测时出现了一个我们训练集中没有的词，那么预测结果为
	\begin{align}
		\phi_{{x_j}|y=1} &= \frac{\sum_{i=1}^{m}1\{x_j^{(i)}=1 \cap y^{(i)}=1\}}{\sum_{i=1}^{m}1\{y^{(i)}=1\}} \\
		&= 0 \\
		\phi_{{x_j}|y=1} &= \frac{\sum_{i=1}^{m}1\{x_j^{(i)}=1 \cap y^{(i)}=0\}}{\sum_{i=1}^{m}1\{y^{(i)}=0\}} \\
		&= 0 \\
		p(y=1|x) &= \frac{\prod_{i=1}^{n}p(x_i|y=1)p(y=1)}{\left[\prod_{i=1}^{n}p(x_i|y=1)\right]p(y=1)+\left[\prod_{i=1}^{n}p(x_i|y=0)\right]p(y=0)} \\
		&= \frac{0}{0}
	\end{align}
	此时出现了$\frac{0}{0}$的情况，没法进行预测。为了解决此问题，我们可以使用拉普拉斯平滑。\footnote{吴恩达的视频中，举了一个已知前面5次比赛都失败，预测第6次失败的概率，可以看下。}
\end{enumerate}

\subsubsection{拉普拉斯平滑介绍}
\begin{enumerate}
	\item $\frac{0}{0}$出现的原因 \\
	由前面$\phi_j = \frac{\sum_{i=1}^{m}1\{z^{(i)}=j\}}{m}$可知，$\phi_j$为出现$z^{(i)}=j$的数据占总数据的比例，在$z^{(i)}=j$在训练集中未出现过时，就会出现$\phi_j=0$\footnote{在$=0$处频数为0，在$=1$处频数也为0}，从而导致出现$\frac{0}{0}$的情况，为了解决此问题，我们队$\phi_j$稍作更改：
	\begin{equation}
		\phi_j = \frac{\sum_{i=1}^{m}1\{z^{(i)}=j\}+1}{m+k}
	\end{equation}
	经过此方式处理后:
	\begin{enumerate}
		\item $\sum_{j=1}^{k}\phi_j = 1$仍旧成立
		\item 对任意$\phi_j \neq 0$
	\end{enumerate}
\end{enumerate}













